<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>人脸识别模型 论文研读 | YTEP</title><meta name="author" content="YTEP"><meta name="copyright" content="YTEP"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="人脸识别模型 论文研读+实验分析 By –B–18–杨佳智  Ⅰ. Content  论文研读   DeepFace: Closing the Gap to Human-Level Performance in Face Verification  FaceNet: A Unified Embedding for Face Recognition and Clustering  Deep Lear">
<meta property="og:type" content="article">
<meta property="og:title" content="人脸识别模型 论文研读">
<meta property="og:url" content="http://example.com/2020/12/03/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/index.html">
<meta property="og:site_name" content="YTEP">
<meta property="og:description" content="人脸识别模型 论文研读+实验分析 By –B–18–杨佳智  Ⅰ. Content  论文研读   DeepFace: Closing the Gap to Human-Level Performance in Face Verification  FaceNet: A Unified Embedding for Face Recognition and Clustering  Deep Lear">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s3.ax1x.com/2020/12/02/D4G0W6.png">
<meta property="article:published_time" content="2020-12-03T04:54:30.000Z">
<meta property="article:modified_time" content="2020-12-03T05:43:47.414Z">
<meta property="article:author" content="YTEP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s3.ax1x.com/2020/12/02/D4G0W6.png"><link rel="shortcut icon" href="https://avatars0.githubusercontent.com/u/48089846?s=400&u=39416cbb82df7df1d209ad313a32b17862577596&v=4"><link rel="canonical" href="http://example.com/2020/12/03/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-12-03 13:43:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://avatars0.githubusercontent.com/u/48089846?s=400&amp;u=39416cbb82df7df1d209ad313a32b17862577596&amp;v=4" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://s3.ax1x.com/2020/12/02/D4G0W6.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">YTEP</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">人脸识别模型 论文研读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-12-03T04:54:30.000Z" title="Created 2020-12-03 12:54:30">2020-12-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-12-03T05:43:47.414Z" title="Updated 2020-12-03 13:43:47">2020-12-03</time></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="人脸识别模型-论文研读-实验分析"><a href="#人脸识别模型-论文研读-实验分析" class="headerlink" title="人脸识别模型 论文研读+实验分析"></a>人脸识别模型 论文研读+实验分析</h2><blockquote>
<p>By –B–18–杨佳智</p>
</blockquote>
<h3 id="Ⅰ-Content"><a href="#Ⅰ-Content" class="headerlink" title="Ⅰ. Content"></a>Ⅰ. Content</h3><hr>
<ul>
<li>论文研读</li>
</ul>
<ol>
<li><p>DeepFace: Closing the Gap to Human-Level Performance in Face Verification</p>
</li>
<li><p>FaceNet: A Unified Embedding for Face Recognition and Clustering</p>
</li>
<li><p>Deep Learning Face Representation from Predicting 10,000 Classes</p>
</li>
<li><p>A Discriminative Feature Learning Approach for Deep Face Recognition</p>
</li>
<li><p>SphereFace: Deep Hypersphere Embedding for Face Recognition</p>
</li>
</ol>
<ul>
<li>实验分析 （评测 GitHub 上公开的人脸识别模型）</li>
</ul>
<ol>
<li><p>FaceNet (Based on Paper2: FaceNet)</p>
</li>
<li><p>DeepID (Based on Paper3)</p>
</li>
<li><p>SphereFace (Based on Paper 5: SphereFace)</p>
</li>
</ol>
<hr>
<h3 id="Ⅱ-论文研读"><a href="#Ⅱ-论文研读" class="headerlink" title="Ⅱ. 论文研读"></a>Ⅱ. 论文研读</h3><h4 id="Paper-1-DeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification"><a href="#Paper-1-DeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification" class="headerlink" title="Paper 1: DeepFace: Closing the Gap to Human-Level Performance in Face Verification"></a>Paper 1: DeepFace: Closing the Gap to Human-Level Performance in Face Verification</h4><p><strong>研读分析：</strong></p>
<p>目前的人脸识别流程通常包括四个阶段，$detect\Rightarrow align \Rightarrow represent \Rightarrow classify$ 。这篇文章主要通过以下两个方面改进当时已有的人脸识别系统：</p>
<ol>
<li><p>在脸部对齐 $(align)$ 的阶段通过建立脸部的 3D 模型以获得更好的对齐效果。选取(g)阶段的图像作为人脸正面图像输入神经网络。</p>
<img src="img1.png" alt="image-20200716092619779" style="zoom:50%;" />
</li>
</ol>
<ol start="2">
<li><p>在特征选择及表达 $(represent)$ 的阶段通过构建深度神经网络提取脸部特征。</p>
<p>当时大多数的人脸识别系统都通过复杂的特征选取流程完成人脸识别任务，但是这些特征对光照，表情，年龄等因素较为敏感，泛化能力不强，在处理无限制的人脸图像时效果较差。同时系统还通常结合人工处理的特征以提高其性能，但是大多数的人脸图像无特定限制，也缺少人工处理。因此本论文使用 DNN，让系统自己在人脸图像中学习泛化能力强的特征，减少人工干预。</p>
</li>
</ol>
<img src="img2.png" alt="image-20200715103720976" style="zoom: 50%;" />

<p>网络输入为经对齐处理后的人脸正面图像，之后分别经过卷积层-池化层-卷积层处理，然后是三个局部连接层和全连接。各层网络的目的：</p>
<table>
<thead>
<tr>
<th align="left">网络层级</th>
<th align="left">目的</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">卷积层-池化层-卷积层</td>
<td align="left">提取底层特征，如：边缘和纹理。</td>
<td>池化层可能会造成一定的信息损失，因此本论文中只采用了一个池化层</td>
</tr>
<tr>
<td align="left">三层局部连接层</td>
<td align="left">获取较为高层的特征。</td>
<td>面部的不同区域的特征分布情况不同，因此对于不同区域采用不同的权重进行学习。</td>
</tr>
<tr>
<td align="left">两层全连接层</td>
<td align="left">找到面部不同特征之间的整体整体联系，并得到最终的特征向量。</td>
<td>近 95%的参数来自局部连接层和全连接层。</td>
</tr>
<tr>
<td align="left">SoftMax 层</td>
<td align="left">根据最终获取的面部特征对该人脸进行归类。</td>
<td>该人脸图像属于类别 k 的概率： $p_k=exp(o_k)/\sum_hexp(o_h)$。</td>
</tr>
</tbody></table>
<p><strong>总结思考：</strong></p>
<ol>
<li><p>该论文最重要的贡献是<strong>首次</strong>在人脸识别问题上较为有效地使用了 DNN 模型，大幅提高了人脸识别系统的准确率。</p>
</li>
<li><p>通过 3D 建模进行人脸对齐，降低了图像中人脸的不同方位，角度所带来的影响。</p>
</li>
<li><p>该文中仍然有可改进的地方，比如该文中损失函数采用的 cross-entropy(交叉熵)，而通过之后的论文阅读可以发现，将损失函数改进为<strong>center-loss</strong>, <strong>triplet-loss</strong>, 或者其他 angular-loss(eg: <strong>sphere loss</strong>)，能更好的实现<strong>类间离散，类内聚集</strong>，使得人脸识别模型准确率更好。</p>
</li>
</ol>
<h4 id="Paper-2-FaceNet-A-Unified-Embedding-for-Face-Recognition-and-Clustering"><a href="#Paper-2-FaceNet-A-Unified-Embedding-for-Face-Recognition-and-Clustering" class="headerlink" title="Paper 2: FaceNet: A Unified Embedding for Face Recognition and Clustering"></a>Paper 2: FaceNet: A Unified Embedding for Face Recognition and Clustering</h4><p><strong>研读分析：</strong></p>
<p>通过 FaceNet 将人脸图像映射到紧密的欧式空间中，欧式空间中的距离即对应着原始图像相似度的度量： 同一个人的人脸图像映射到欧式空间后距离较小，而不同人的图像映射后距离较大。经过这种映射处理后，关于人脸图像的以下问题便可以作进一步的简化：</p>
<p>a. 人脸验证 ($face\ verification$) $\Rightarrow$ 判断两点距离是否达到不同类别的阈值。</p>
<p>b. 人脸识别($face\ recognition$) $\Rightarrow$ k-NN 算法进行分类。</p>
<p>c. 人脸图像聚类($face \ clustering$) $\Rightarrow$ k-means 算法或者 agglomerative 聚类算法。</p>
<p>本文的<strong>核心贡献</strong>是设计特殊的<strong>损失函数</strong> (Triplet Loss)，使得人脸图像经过映射后满足上述要求：同一人的人脸映射后距离较小，不同人的人脸映射后距离较大，同时存在一个距离的阈值用于区分两个图像是否属于同一人（距离小于阈值–图像属于同一人，距离大于阈值–图像属于不同的人）。</p>
<img src="img3.png" alt="image-20200715161850934" style="zoom: 67%;" />

<img src="img4.png" alt="image-20200715161910834" style="zoom: 67%;" />

<p>数学表示： 对于图像映射的表示: $image \ (x) \xrightarrow  {embedding} feature \ vector ( f(x)) ,\ ||f(x)||_2 =1$ 。给定任一个人的图像 $x_i^a(anchor)$, 同一个人的其他图像 $x_i^p(positive)$, 其他人的图像 $x_i^n(negative)$。</p>
<p>我们希望其满足：$||f(x_i^a)-f(x_i^p)||_2^2+\alpha&lt;||f(x_i^a)-f(x_i^n)||_2^2 \ \ \ \ \ (1)$ ，$\alpha$ 表示$margin$, 用于分隔同类图像与不同类图像。因此，损失函数（Triplet Loss）可以表示为：</p>
<p>$$<br>\sum_i^N[\ ||f(x_i^a)-f(x_i^p)||_2^2-||f(x_i^a)-f(x_i^n)||<em>2^2+\alpha\ ]</em>{+} \ \ \ \ \ \  (2)<br>$$</p>
<p>通过最小化 Triplet Loss，便可以更新网络中的参数。但是目前存在的问题是，从一个大小为 N 的数据集中选取三元组并计算，将会产生大量的计算消耗。并且其中部分三元组以及满足了(1)式的要求，会直接通过网络而不会使得网络权重产生更新，因此我们应该采用特殊的方法选择合适的三元组。</p>
<p>本文中采用的方法是：每一次计算 Loss 时，在当前的 mini-batch 中选择 hard positive $(argmax_{x_i^p}||f(x_i^a)-f(x_i^p)||<em>2^2)$ 和 hard negative $(argmin</em>{x_i^n}||f(x_i^a)-f(x_i^n)||_2^2)$ 的样例，用这些样例以及当前的 $x_i^a$ 组成的三元组进行计算 。这样便有效减少了无用的三元组的计算。</p>
<p><strong>总结思考：</strong></p>
<ol>
<li>该模型将人脸图像映射到欧式空间中，构建三元组并计算 Triplet Loss。Triplet Loss 有效的减少了同一个人的人脸图像经映射后的距离，增加了不同人的人脸图像的距离。使得映射后的欧式空间中满足类内聚集，类间离散的特点，有效的增进了该模型进行人脸验证，识别，聚类等任务的准确度。</li>
<li>该模型的缺点：三元组的引入大幅增加了计算消耗，根据原文的说法，该模型在 CPU 上的训练时间为 1000-2000 小时，时间复杂度过高，仍有改进的空间。</li>
</ol>
<h4 id="Paper-3-Deep-Learning-Face-Representation-from-Predicting-10-000-Classes"><a href="#Paper-3-Deep-Learning-Face-Representation-from-Predicting-10-000-Classes" class="headerlink" title="Paper 3: Deep Learning Face Representation from Predicting 10,000 Classes"></a>Paper 3: Deep Learning Face Representation from Predicting 10,000 Classes</h4><p><strong>研读分析：</strong></p>
<p>本文的主要贡献是，设计 CNN 以提取面部不同位置的高层特征表达。之前的神经网络对于某一图像输入都只针对这张图像本身进行特征提取；而在本文的模型中，为了提取更加完整的特征，作者将同一张图片按照 10 个不同的面部区域，3 种不同的规模大小，RGB 或者是灰度通道分别送入 60 个神经网络，将各网络得到的特征向量进行拼接，最终得到 19200(160 x 2 x 60)维的表示该人脸图像的特征向量。该特征向量能更加完备的反映原始人脸图像。</p>
<img src="img5.png" alt="image-20200715215735241" style="zoom: 33%;" />

<p>如上图所示，上半部分表示人脸的 10 个不同的区域，下半部分为两个 patch 的 3 种不同规模大小。</p>
<p>网络配置如同下图所示：</p>
<img src="img6.png" alt="image-20200715220108695" style="zoom: 50%;" />

<p>其中前三个卷积-池化层是为了逐层获取更高级更抽象的特征，最后的特征提取层(最终特征称为 DeepID，将该层简称为 ID 层) 与第三个池化层以及第四个卷积层全连接，因此 ID 层能够处理两种不同规模的脸部特征。将 ID 层同时与第三个池化层以及第四个卷积层连接提高了模型的性能，因为图片经过了逐层的下采样，有一定的面部信息损失，如果只连接第四层，则面部信息损失较大；而同时连接第三个池化层可以减少信息缺失带来的影响。</p>
<p><strong>总结思考：</strong></p>
<ol>
<li>该模型将每个人脸图像，都按照不同的面部位置，不同的规模大小，以及不同的颜色通道转化为不同的输入，经过多个 CNN 处理后，将各个得到的特征进行拼接，最终得到高维的较完备的脸部特征。但是这种方法的缺点是时间成本太高，为了分析各部分特征，作者对上述不同的输入(共 60 种)训练了 60 个不同的 CNN 网络，这大大增加了计算成本，是需要改进的地方。</li>
<li>该模型对脸部对齐程度的要求不高(only requiring <u>weakly aligned faces</u>)。</li>
<li>该模型将最后的特征提取层与第四个卷积层和第三个池化层全连接，有效的减少了逐层下采样带来的信息损耗。这个设计可以算是这篇文章的一个创新点，之后的许多人脸识别模型都沿用了这一设计方式。在最早的 Paper1: DeepFace 中, 为了减小信息损失，其作者采用的方法是直接减少 maxpooling-layer 的使用，只在第一层使用了池化。我们可以考虑对比这两种不同的方案，以设计更高效的网络。</li>
</ol>
<h4 id="Paper-4-A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition"><a href="#Paper-4-A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition" class="headerlink" title="Paper 4: A Discriminative Feature Learning Approach for Deep Face Recognition"></a>Paper 4: A Discriminative Feature Learning Approach for Deep Face Recognition</h4><p><strong>研读分析：</strong></p>
<img src="img7.png" alt="image-20200715183109968" style="zoom: 50%;" />

<p>当时大多数已有的人脸识别模型都是采用 softmax loss 作为损失函数，但是 softmax loss 只能使网络区分不同的特征 (encourage the separability of features)，但是无法增加特征的识别力(discriminative power)。discriminative power 对于人脸检验，判别<u>不在训练集中</u>的未知人脸的类别，以及人脸聚类问题都相当重要 。因此，本文设计了 center loss, 该损失函数能有效增强人脸图像特征的 discriminative power，即其能增加类间的离散程和类内的紧密程度。</p>
<p>与 triplet loss 的对比：Paper 2(FaceNet)中提到的 triplet loss 同样能增强特征的 discriminative power，但是 triplet loss 在网络中的计算量太大，选择合适的三元组的过程较为复杂，不利于实现 。因此本文中的 center loss 以其简易和轻量的特点占据一定的优势。</p>
<p>具体实现方法：</p>
<p>$$<br>L_c = \frac{1}{2}\sum_{i=1}^m||x_i-c_{y_i}||_2^2 \ \ \ \ \ \ \ \ \ \ \  (3)<br>$$</p>
<p>$c_{y_i}$表示类别${y_i}$的特征中心。$Lc$ 反映了类内的紧密程度，降低$Lc$即使得类内更加聚集。之后我们再将 center loss 与 softmax loss 相结合得到最终用于训练的损失函数:</p>
<p>$$<br>L = L_s+L_c =-\sum_{i=1}^mlog\frac{e^{W^T_{y_i}x_i+b_{y_i}}}{\sum_{j=1}^ne^{W_j^Tx_i+b_j}}+\frac{\lambda}{2}\sum_{i=1}^m||x_i-c_{y_i}||_2^2<br>$$</p>
<p>其中，控制$L_c$部分的系数$\lambda$ 的取值不同会导致得到的特征分布不同。通过选取适当的$\lambda$，我们可以得到较为理想的特征分布。</p>
<img src="img8.png" alt="image-20200715212810863" style="zoom:50%;" />

<p>算法流程如下：</p>
<img src="img9.png" alt="image-20200715192110443" style="zoom: 50%;" />

<p>网络设计如下：</p>
<img src="img10.png" alt="image-20200715213050017" style="zoom: 67%;" />

<p><strong>总结思考：</strong></p>
<ol>
<li><p>其实本文的思路可以理解为：在原有的 softmax loss 的基础上增加一个惩罚项(即文中的(center loss)，该惩罚项控制类内的聚集程度。</p>
</li>
<li><p>center loss 中不同类别的特征中心不断迭代更新的思想与聚类算法 k-means 有一定的相似性。(后者是不断迭代更新每个子集的质心，再将各点划分到距离最近的质心所属的类别)。</p>
</li>
<li><p>center loss 的思想便于理解，并且实现简单，计算复杂度较低，其对应的 CNN 网络较为轻量。</p>
</li>
</ol>
<h4 id="Paper-5-SphereFace-Deep-Hypersphere-Embedding-for-Face-Recognition"><a href="#Paper-5-SphereFace-Deep-Hypersphere-Embedding-for-Face-Recognition" class="headerlink" title="Paper 5: SphereFace: Deep Hypersphere Embedding for Face Recognition"></a>Paper 5: SphereFace: Deep Hypersphere Embedding for Face Recognition</h4><p><strong>研读分析：</strong></p>
<p>本文提出了一种 angular softmax loss – A-Softmax loss, 使得 CNN 能够学习与角度相关的有识别力的特征(angularly discriminative features)。通过应用该损失函数，人脸图像特征可以被映射到超球面上，这种映射可以使得不同类别与各决策面的距离越大，类内数据更为聚集，同时类间数据更加离散。这将更有利于人脸识别性能的提升。</p>
<p>同时通过设定参数 m，我们可以对类别间角度的分隔(angular margin)做量化分析。</p>
<p>数学表示：</p>
<p>对于一个二分类的例子，通过 softmax 计算得到$x\in c_1 $ 或者 $c_2$ 的概率分别是：</p>
<img src="img11.png" alt="image-20200716002847839" style="zoom: 60%;" />

<img src="img12.png" alt="image-20200716002926408" style="zoom:60%;" />

<img src="img13.png" alt="image-20200716002944274" style="zoom: 67%;" />

<img src="img14.png" alt="image-20200716003006192" style="zoom: 67%;" />

<img src="img15.png" alt="image-20200716005021938" style="zoom: 67%;" />

<img src="img16.png" alt="image-20200716005036810" style="zoom:67%;" />

<p>分别应用 Original Softmax loss, modified Softmax loss, 以及 A-softmax loss 完成下图中两种数据点的分类，可以看到，应用 Original Softmax loss 的情况下，分界面周围仍然有大量错分的数据点。而使用 Modified Softmax loss 后，数据点的类别已经能相对正确的划分开，但是划分结果缺乏识别力(discriminative power)，即类间与类内数据点的区分程度不够。最后使用 A-softmax，可以看到类别间能被很好的间隔开，同时类内的各数据点分布也相对密集，这有利于提高识别，检测等操作的准确率。不同类别之间的分隔由参数 m 确定。</p>
<img src="img17.png" alt="image-20200716005036810" style="zoom:67%;" />

<p>下图是对 A-softmax loss 采用不同的 m，特征空间中不同类别特征的分布情况。由下图可见，m=4 时特征的识别力更强。</p>
<img src="img18.png" alt="image-20200716070821727" style="zoom: 67%;" />

<p><strong>总结思考：</strong></p>
<ol>
<li><p>这是首篇成功地在人脸识别模型中利用 angular margin 增强模型性能的文章。并且通过实验测试结果与可视化展示了 A-softmax loss 的有效性。</p>
</li>
<li><p>A-softmax loss 计算量小于 triplet loss, 且识别力比 center loss 更好，成为了当前常用的一种损失函数。之后的许多工作，eg: CosFace, ArcFace 等都是在本文核心思想的基础上开展的工作。</p>
</li>
</ol>
<h3 id="Ⅲ-实验分析"><a href="#Ⅲ-实验分析" class="headerlink" title="Ⅲ. 实验分析"></a>Ⅲ. 实验分析</h3><ol>
<li>FaceNet</li>
</ol>
<p>FaceNet 是基于 Paper 2: FaceNet 的开源人脸识别系统。FaceNet 给使用者提供了两个预训练的模型（在 Paper 2 的分析中提到，由于 triplet loss 的计算量太大，FaceNet 的训练时间很长–论文中的描述是 1000-2000 小时，因此我们选择直接采用官网上的预训练模型进行检测。）</p>
<p>这两个预训练模型分别是在 CASIA-WebFace 以及 VGGFace2 数据集上进行的训练。</p>
<img src="img19.png" alt="image-20200716072616010" style="zoom:50%;" />

<p>我们使用预训练模型二并在 LFW 数据集上进行测试。首先使用提供的 align_dataset_mtcnn.py，对原始图片数据进行对齐，得到规范化后的数据集。然后运行 validate_on_lfw.py，对规范化后的数据集进行测试，得到结果如下：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>数值</th>
</tr>
</thead>
<tbody><tr>
<td>准确率 (Accuracy)</td>
<td>0.99650+-0.00252</td>
</tr>
<tr>
<td>误识率(FAR)</td>
<td>0.001</td>
</tr>
<tr>
<td>等错误率(EER)</td>
<td>0.04</td>
</tr>
</tbody></table>
<ol start="2">
<li>DeepID</li>
</ol>
<p>基于 Paper 3: DeepID 进行设计开发，注：该库并非由原作者开发，而是复现效果较好的第三方库。</p>
<p>在文章中，作者为了增进模型识别准确率，人为的扩充了数据集。（CeleFace $\Rightarrow $CeleFace+ (含有 10177 个名人的 202599 张人脸图像)），由于原作者未公开这部分扩充的数据集，因此这个第三方库采用了其他数据集(Youtube face)进行模型的训练和测试。</p>
<p>首先运行 youtube_img_crop.py 对人脸图像进行剪裁，然后将图像向量化再送入网络，运行 deep_id_generate.py 得到所有图像的特征向量。最后运行 deep_class.py 对测试集中的人脸数据进行分类。最终的测试结果为：在 Youtube face 测试集部分的准确率为 **97.69%**。</p>
<ol start="3">
<li>Sphereface</li>
</ol>
<p>该系统包含人脸识别的整个流程(人脸检测，人脸对齐以及人脸识别)，该系统是在 CAISA-WebFace 上进行的模型预训练。我们可以使用预训练的模型在 LFW 数据集上进行测试 。</p>
<p>运行 face_detect.m 以及 face_align.m 将 LFW 中人脸图像数据进行检测以及对齐操作。之后执行 sphereface_train.sh 训练球面模型。最后运行 evaluation.m 得到准确率。最终的实验得到的准确率为**99.29%**。</p>
<p>补充： Sphereface 库还可以对视频中的人脸进行实时检测，如下所示（对不同的人脸用了不同颜色的框图进行区分）：</p>
<img src="img20.png" alt="image-20200716092254020" style="zoom:50%;" /></article><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://s3.ax1x.com/2020/12/02/D4G0W6.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2020/12/03/image-text-matching-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/"><img class="prev-cover" src="https://s3.ax1x.com/2020/12/02/D4G1zT.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">image-text matching 论文研读</div></div></a></div></nav></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://avatars0.githubusercontent.com/u/48089846?s=400&amp;u=39416cbb82df7df1d209ad313a32b17862577596&amp;v=4" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">YTEP</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">5</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YTEP-ZHI"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/YTEP-ZHI" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/ytepjz@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB-%E5%AE%9E%E9%AA%8C%E5%88%86%E6%9E%90"><span class="toc-number">1.</span> <span class="toc-text">人脸识别模型 论文研读+实验分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%85%A0-Content"><span class="toc-number">1.1.</span> <span class="toc-text">Ⅰ. Content</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%85%A1-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB"><span class="toc-number">1.2.</span> <span class="toc-text">Ⅱ. 论文研读</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Paper-1-DeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification"><span class="toc-number">1.2.1.</span> <span class="toc-text">Paper 1: DeepFace: Closing the Gap to Human-Level Performance in Face Verification</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Paper-2-FaceNet-A-Unified-Embedding-for-Face-Recognition-and-Clustering"><span class="toc-number">1.2.2.</span> <span class="toc-text">Paper 2: FaceNet: A Unified Embedding for Face Recognition and Clustering</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Paper-3-Deep-Learning-Face-Representation-from-Predicting-10-000-Classes"><span class="toc-number">1.2.3.</span> <span class="toc-text">Paper 3: Deep Learning Face Representation from Predicting 10,000 Classes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Paper-4-A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition"><span class="toc-number">1.2.4.</span> <span class="toc-text">Paper 4: A Discriminative Feature Learning Approach for Deep Face Recognition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Paper-5-SphereFace-Deep-Hypersphere-Embedding-for-Face-Recognition"><span class="toc-number">1.2.5.</span> <span class="toc-text">Paper 5: SphereFace: Deep Hypersphere Embedding for Face Recognition</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%85%A2-%E5%AE%9E%E9%AA%8C%E5%88%86%E6%9E%90"><span class="toc-number">1.3.</span> <span class="toc-text">Ⅲ. 实验分析</span></a></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/02/22/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97Task1/" title="No title"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/2021/02/22/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97Task1/" title="No title">No title</a><time datetime="2021-02-22T02:51:57.353Z" title="Created 2021-02-22 10:51:57">2021-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/19/Task1%E6%89%93%E5%8D%A1/" title="No title"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/2021/02/19/Task1%E6%89%93%E5%8D%A1/" title="No title">No title</a><time datetime="2021-02-19T13:05:52.157Z" title="Created 2021-02-19 21:05:52">2021-02-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/10/Stack%20Cross%20Attention%20for%20Image-Text%20matching/" title="Stacked Cross Attention for Image-Text Matching"><img src="https://ftp.bmp.ovh/imgs/2020/12/70af6e31e1ffa15c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Stacked Cross Attention for Image-Text Matching"/></a><div class="content"><a class="title" href="/2020/12/10/Stack%20Cross%20Attention%20for%20Image-Text%20matching/" title="Stacked Cross Attention for Image-Text Matching">Stacked Cross Attention for Image-Text Matching</a><time datetime="2020-12-10T08:50:58.000Z" title="Created 2020-12-10 16:50:58">2020-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/03/image-text-matching-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="image-text matching 论文研读"><img src="https://s3.ax1x.com/2020/12/02/D4G1zT.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="image-text matching 论文研读"/></a><div class="content"><a class="title" href="/2020/12/03/image-text-matching-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="image-text matching 论文研读">image-text matching 论文研读</a><time datetime="2020-12-03T04:55:58.000Z" title="Created 2020-12-03 12:55:58">2020-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/03/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="人脸识别模型 论文研读"><img src="https://s3.ax1x.com/2020/12/02/D4G0W6.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="人脸识别模型 论文研读"/></a><div class="content"><a class="title" href="/2020/12/03/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="人脸识别模型 论文研读">人脸识别模型 论文研读</a><time datetime="2020-12-03T04:54:30.000Z" title="Created 2020-12-03 12:54:30">2020-12-03</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By YTEP</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>